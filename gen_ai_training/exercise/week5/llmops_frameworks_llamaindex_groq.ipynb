{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qgb5DqmU-OAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interacting with LLM with LLamaINDEX Framework"
      ],
      "metadata": {
        "id": "_mNDKKdi-OCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries and set API key"
      ],
      "metadata": {
        "id": "Y8QJXg-8-Wlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU llama-index-llms-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYaGi2SQ-S9P",
        "outputId": "cad4451d-aa7e-4d3c-9e73-d54ebfaa8008"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.6 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU llama-index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P35GoUAK-jVb",
        "outputId": "f815494c-8383-446f-d375-9a40b37f3144"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/248.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.0/248.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.groq import Groq"
      ],
      "metadata": {
        "id": "PXW9EhFR-t7X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "MjFTNC8w-6tq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Groq(model=\"llama3-70b-8192\")"
      ],
      "metadata": {
        "id": "P6NAk2AY_TZj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"What is the capital of France\""
      ],
      "metadata": {
        "id": "5rxts6lr_fBZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.complete(prompt)"
      ],
      "metadata": {
        "id": "CFYRye_W_lbu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwBAMw51_o2N",
        "outputId": "fa4446f2-b96c-4bc0-f479-28c10bf73a6a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Call `chat` with a list of messages"
      ],
      "metadata": {
        "id": "avoXr3Mz_xHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "messages = [\n",
        "    ChatMessage(role=\"system\", content=\"You are a pirate with a colorful personality.\"),\n",
        "    ChatMessage(role=\"user\", content=\"Tell me a story.\"),\n",
        "]\n",
        "resp = model.chat(messages)"
      ],
      "metadata": {
        "id": "Nhf7Exxy_rN_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skaGBPXWAImN",
        "outputId": "1dd8ec7b-9a1a-477a-b14c-f6eada24410f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: Arrr, gather 'round me hearties! Ol' Blackbeak Betty be tellin' ye a tale of adventure, danger, and a wee bit o' luck!\n",
            "\n",
            "It were a dark and stormy night, the kind that makes ye want to stay in yer hammock with a good bottle o' rum. But I weren't havin' none o' that! Me and me trusty crew, the Scurvy Dogs, had received a tip about a Spanish galleon carryin' a cargo o' gold doubloons and sparklin' gems. We set sail fer the island o' Tortuga, where the galleon were supposed to be hidin' out.\n",
            "\n",
            "As we sailed through the choppy waters, the winds howlin' and the lightnin' flashin' across the sky, I could feel the excitement buildin' in me bones! Me crew were a rough bunch, but we worked together like a well-oiled machine, each one o' us knowin' our part in the plan.\n",
            "\n",
            "We spied the galleon anchored in a hidden cove, its sails torn and its hull battered from the storm. I gave the signal, and me crew launched the longboats, ready to board the Spanish ship. I led the charge, me cutlass flashin' in the moonlight as we swarmed up the sides o' the galleon.\n",
            "\n",
            "The Spanish sailors put up a fight, but we were too fierce, too cunning! We battled our way across the decks, takin' down the enemy and claimin' the treasure as our own. And what a haul it were! Gold and jewels sparklin' like the stars in the night sky, glintin' in the lantern light as we loaded it onto our own ship.\n",
            "\n",
            "But just as we were makin' our escape, the storm took a turn fer the worse! A great wave crashed over the bow, threatenin' to send us all to Davy Jones' locker! Me crew were clingin' to the riggin' fer dear life, and I were shoutin' orders to keep us afloat.\n",
            "\n",
            "That's when I saw him, the Spanish captain, a fierce and proud man with a sword in his hand and a fire in his eye. He challenged me to a duel, right there on the deck o' the galleon! I accepted, o' course, and we clashed swords in the ragin' storm.\n",
            "\n",
            "It were a fierce battle, the two o' us dancin' across the deck, our blades flashin' and clashin' in the lightnin' flashes. But in the end, it were me who emerged victorious, me cutlass pressed to the captain's throat.\n",
            "\n",
            "\"Yield,\" I growled, me teeth gritted against the wind.\n",
            "\n",
            "The captain nodded, and I spared his life. We took the treasure and set sail fer the open sea, the storm beginnin' to subside as we sailed into the dawn.\n",
            "\n",
            "And that, me hearties, be the tale o' how Ol' Blackbeak Betty and the Scurvy Dogs claimed the greatest treasure o' the seven seas! Arrr!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streaming"
      ],
      "metadata": {
        "id": "8uTBUcTzAV4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lFv28m3wAai_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using `stream_complete` endpoint"
      ],
      "metadata": {
        "id": "OjoTDWoBAak7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.stream_complete(\"Explain the importance of low latency LLMs\")"
      ],
      "metadata": {
        "id": "zYP921HCARGU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for r in response:\n",
        "  print(r.delta, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFb1Z5ysAoz0",
        "outputId": "8dd1ed4b-a5bf-4aa2-ecc9-dc3e44518abb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Low-latency Large Language Models (LLMs) are crucial in various applications where real-time or near-real-time processing is essential. Here are some reasons why low-latency LLMs are important:\n",
            "\n",
            "1. **Conversational AI**: In conversational AI, such as chatbots, voice assistants, and virtual assistants, low-latency LLMs enable rapid response times, making the interaction feel more natural and human-like. This is critical in applications like customer service, where timely responses can significantly impact user experience and satisfaction.\n",
            "2. **Real-time Language Translation**: Low-latency LLMs can facilitate real-time language translation, enabling instant communication across language barriers. This is particularly important in applications like video conferencing, live subtitles, and simultaneous interpretation.\n",
            "3. **Speech Recognition**: Fast and accurate speech recognition relies on low-latency LLMs. This is essential in applications like voice-controlled devices, voice assistants, and transcription services, where rapid response times are critical.\n",
            "4. **Gaming and Interactive Systems**: In gaming and interactive systems, low-latency LLMs can enhance the user experience by enabling rapid response times, more realistic NPC interactions, and more engaging storytelling.\n",
            "5. **Healthcare and Emergency Services**: In healthcare and emergency services, low-latency LLMs can facilitate rapid diagnosis, treatment, and response times. For example, AI-powered medical chatbots can quickly provide medical information and guidance, while emergency response systems can rapidly analyze and respond to emergency calls.\n",
            "6. **Autonomous Systems**: In autonomous systems, such as self-driving cars and drones, low-latency LLMs can enable rapid decision-making and response times, which are critical for safety and efficiency.\n",
            "7. **Edge Computing**: With the increasing adoption of edge computing, low-latency LLMs can be deployed closer to the user, reducing latency and improving real-time processing capabilities.\n",
            "8. **Improved User Experience**: Low-latency LLMs can significantly improve the user experience in various applications, such as:\n",
            "\t* Faster search results and query processing\n",
            "\t* Rapid content generation and recommendation\n",
            "\t* Enhanced augmented reality (AR) and virtual reality (VR) experiences\n",
            "\t* Smoother and more responsive human-computer interactions\n",
            "9. **Competitive Advantage**: In many industries, low-latency LLMs can provide a competitive advantage by enabling faster and more efficient processing, leading to improved customer satisfaction, increased revenue, and market leadership.\n",
            "10. **Research and Development**: Low-latency LLMs can accelerate research and development in various fields, such as natural language processing, computer vision, and robotics, by enabling faster experimentation, prototyping, and testing.\n",
            "\n",
            "In summary, low-latency LLMs are essential in applications where rapid processing and response times are critical, such as conversational AI, real-time language translation, speech recognition, gaming, healthcare, and autonomous systems. They can improve user experience, provide a competitive advantage, and accelerate research and development in various fields."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using `stream_chat` endpoint"
      ],
      "metadata": {
        "id": "7FvjLAQ5BBtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "messages = [\n",
        "    ChatMessage(role=\"system\", content=\"You are an Assistant\"),\n",
        "    ChatMessage(role=\"user\", content=\"Explain the importance of low latency LLMs\"),\n",
        "]\n",
        "resp = model.stream_chat(messages)"
      ],
      "metadata": {
        "id": "tt-SjMmXAyZ1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for r in resp:\n",
        "  print(r.delta, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzCrQzDcBZtR",
        "outputId": "d2723215-1014-41d3-9f82-95b9fe45243e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Low-latency Large Language Models (LLMs) are crucial in various applications where real-time or near-real-time processing is essential. Here are some reasons why low-latency LLMs are important:\n",
            "\n",
            "1. **Real-time Conversational AI**: In conversational AI, such as chatbots, voice assistants, and customer service platforms, low-latency LLMs enable rapid response times, making interactions feel more natural and human-like. This is particularly important in applications where users expect immediate responses, such as customer support or virtual assistants.\n",
            "2. **Live Translation and Subtitling**: Low-latency LLMs can facilitate real-time language translation and subtitling, enabling seamless communication across language barriers. This is critical in applications like live conferences, meetings, or video streaming, where timely translation is essential.\n",
            "3. **Speech Recognition and Synthesis**: Fast and accurate speech recognition and synthesis rely on low-latency LLMs. This is vital in applications like voice-controlled devices, voice assistants, and speech-to-text systems, where rapid processing is necessary for a smooth user experience.\n",
            "4. **Gaming and Interactive Systems**: In gaming and interactive systems, low-latency LLMs can enhance the user experience by enabling rapid processing of natural language inputs, allowing for more immersive and responsive interactions.\n",
            "5. **Healthcare and Emergency Services**: In healthcare and emergency services, timely and accurate language processing can be a matter of life and death. Low-latency LLMs can facilitate rapid diagnosis, treatment, and communication, saving precious time in critical situations.\n",
            "6. **Autonomous Systems and Robotics**: In autonomous systems and robotics, low-latency LLMs can enable real-time processing of natural language inputs, allowing for more efficient and effective decision-making in applications like self-driving cars, drones, or robots.\n",
            "7. **Edge Computing and IoT**: With the proliferation of edge computing and IoT devices, low-latency LLMs can process language inputs locally, reducing latency and improving responsiveness in applications like smart home devices, wearables, or industrial automation systems.\n",
            "8. **Improved User Experience**: Low-latency LLMs can significantly enhance the user experience by providing rapid responses, reducing frustration, and increasing engagement. This is particularly important in applications where users expect instant gratification, such as social media, messaging apps, or online search.\n",
            "9. **Competitive Advantage**: In many industries, low-latency LLMs can provide a competitive advantage by enabling faster and more accurate language processing, leading to improved customer satisfaction, increased revenue, and market differentiation.\n",
            "10. **Future-Proofing**: As language models continue to evolve and become more pervasive, low-latency LLMs will be essential for future applications that require real-time or near-real-time processing, such as augmented reality, virtual reality, or brain-computer interfaces.\n",
            "\n",
            "In summary, low-latency LLMs are critical in various applications where rapid processing of natural language inputs is essential. They can improve user experience, enable real-time decision-making, and provide a competitive advantage in many industries."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9VoJtInOBcvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7-JKyl_K-Wjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nLXBH6OKBkgr"
      }
    }
  ]
}